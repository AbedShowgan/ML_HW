{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g02uyRKMJdL_"
   },
   "source": [
    "# **Ex1 - Unsupervised learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSqx1-GWJcDY"
   },
   "source": [
    "## Names and IDs\n",
    "\n",
    "1.   212112106, Abed Shogan\n",
    "2.   209969245, Noam Shani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fdcpdvZMdJs"
   },
   "source": [
    "**Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E565M8V0JiPx"
   },
   "source": [
    "In this assignment, we will focus on the practical application of unsupervised learning methods, specifically K-means clustering and Principal Component Analysis (PCA). The primary objective is to deepen your understanding of these algorithms and develop proficiency in their implementation using Python and relevant libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmZgvCtuMgfM"
   },
   "source": [
    "**Learning Objectives:**\n",
    "1.   **Load Local Files**: Implement techniques for\n",
    "     loading datasets from a local file system into Python.\n",
    "3.   **Data Visualization**: Apply various visualization techniques to interpret and present your data analysis findings effectively.\n",
    "4.   **Use Scikit-learn for K-means Clustering**: Use the Scikit-learn library to apply the K-means clustering algorithm.\n",
    "5.   **Implement Scikit-learn PCA**: Utilize PCA from Scikit-learn to perform dimensionality reduction, a critical technique for analyzing high-dimensional data.\n",
    "6.   **Algoritmic Understendig**: Solve the calculation problem whle using the algorithms learned in class.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d35uo61hMmPG"
   },
   "source": [
    "\n",
    "**Important Guidelines:**\n",
    "\n",
    "**Thoroughly Read the Task Before Implementation:** Ensure to understand the entire assignment and its requirements before beginning to code. A comprehensive understanding will aid in a more structured and efficient approach to the tasks.\n",
    "\n",
    "**Code Reusability and Function Writing:** Focus on writing reusable code and functions. This practice is crucial for maintaining an organized, efficient, and easily debuggable codebase.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQqIK7IkOXUK"
   },
   "source": [
    "This assignment is designed to enhance both your theoretical understanding and practical skills in key areas of machine learning. Approach each task with diligence and attention to detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Esbc-ZiCO60h"
   },
   "source": [
    "## Import All Packages\n",
    "Add all imports needed for this notebook to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LLufnb-KI64u"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.compose import make_column_transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8ssPm39Q5-6"
   },
   "source": [
    "## 1. Visualization (15 points)\n",
    "In this section, your task is to create and analyze **three** insightful visualizations based on the customer segmentation dataset. The purpose of these visualizations is to uncover underlying patterns and trends in the data that can inform strategic decisions. Your ability to interpret these visualizations will be key in understanding customer behaviors and preferences.\n",
    "\n",
    "*   You will get 4 points for the graph and 1 for the insight.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lwd7T5K-Q5ap"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Gal.Shani\\\\Documents\\\\Noam files\\\\Machine Learning\\\\HW_1\\\\customer_segmentation.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mGal.Shani\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mNoam files\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mMachine Learning\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mHW_1\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mcustomer_segmentation.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# a = 5\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# # Load data into a DataFrame\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m data_frame \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(path)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Get the size of the DataFrame\u001b[39;00m\n\u001b[0;32m      8\u001b[0m rows, columns \u001b[38;5;241m=\u001b[39m data_frame\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1706\u001b[0m     f,\n\u001b[0;32m   1707\u001b[0m     mode,\n\u001b[0;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1714\u001b[0m )\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Gal.Shani\\\\Documents\\\\Noam files\\\\Machine Learning\\\\HW_1\\\\customer_segmentation.csv'"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "# Provide the path to your Excel file\n",
    "path = 'C:\\\\Users\\\\Gal.Shani\\\\Documents\\\\Noam files\\\\Machine Learning\\\\HW_1\\\\customer_segmentation.csv'\n",
    "# a = 5\n",
    "# # Load data into a DataFrame\n",
    "data_frame = pd.read_csv(path)\n",
    "# Get the size of the DataFrame\n",
    "rows, columns = data_frame.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q01Pg0E4aGZd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(a)\n",
    "current_year = 2024\n",
    "data_frame['age'] = current_year - data_frame['Year_Birth']\n",
    "\n",
    "    # Define age categories\n",
    "bins = [0, 18, 35, 55, 75, data_frame['age'].max()]\n",
    "labels = ['0-18', '18-35', '35-55', '55-75', '75+']\n",
    "\n",
    "    # Add a new column for age category\n",
    "data_frame['age_category'] = pd.cut(data_frame['age'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "    # Create a bar plot\n",
    "plt.figure(figsize=(4, 3))\n",
    "sns.countplot(x='age_category', data=data_frame, palette='viridis')\n",
    "plt.title('Number of People by Age Category')\n",
    "plt.xlabel('Age Category')\n",
    "plt.ylabel('Number of People')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KDC-WccATcUJ"
   },
   "outputs": [],
   "source": [
    "# Graph 2 -     #Sum the amounts for each food category across all customers\n",
    "category_totals = data_frame[['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts','MntGoldProds']].sum()\n",
    "\n",
    "#dictionary to map original category names to custom labels\n",
    "category_labels = {\n",
    "    'MntWines': 'Wines',\n",
    "    'MntFruits': 'Fruits',\n",
    "    'MntFishProducts': 'Fish',\n",
    "    'MntMeatProducts': 'Meats',\n",
    "    'MntSweetProducts': 'Sweets',\n",
    "    'MntGoldProds': 'Gold'\n",
    "    }\n",
    "\n",
    "    #Map original category names to my labels\n",
    "custom_labels = [category_labels.get(category, category) for category in category_totals.index]\n",
    "\n",
    "    # Plot a pie chart with custom labels\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(category_totals, labels=custom_labels, autopct='%1.1f%%', startangle=140,\n",
    "        colors=['lightcoral', 'skyblue', 'lightgreen', 'gold', 'lightpink','black'])\n",
    "plt.title('Amount Spent Per Food Category')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8sWW3XfQX5_Y"
   },
   "source": [
    "**Insight** -  The Insight we can take from Graph 1 is that most Customers are between the Ages of 35 and 75, We have Some elderly over the age of 75 and some who are young (18 to 35) We Also do not have under age Customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Luq8ewBWWsX"
   },
   "source": [
    "**Insight** - In Graph 2  we can understand that the most amount of money spent by customers is on Wines, Above 50% of all spendings!, After that comes in Meat products and lastly are Fish Products, which customers spend the least on.\n",
    "We can see the Distributions of Ratios in This Pie Chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4jIXZXKSTd4w",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Graph 3 -      # Group the data by education level and calculate the counts of responders and non-responders\n",
    "education_counts = data_frame.groupby(['Education', 'Response']).size().unstack().fillna(0)\n",
    "\n",
    "        # Reset the index to make 'Education' and 'Response' regular columns\n",
    "education_counts = education_counts.reset_index()\n",
    "\n",
    "        # Melt the DataFrame\n",
    "melted_data = pd.melt(education_counts, id_vars='Education', var_name='Response', value_name='Count')\n",
    "        # Plot a bar chart\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x='Education', y='count', hue='Response',\n",
    "                data=education_counts.reset_index().melt(id_vars=['Education'], var_name='Response',\n",
    "                                                            value_name='count'))\n",
    "plt.title('Responder Distribution by Education Level')\n",
    "plt.legend(title='Response', labels=['Non-Responder - Orange', 'Responder - Blue'])\n",
    "plt.xlabel('Education ')\n",
    "plt.ylabel('Amount Responded')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uf0OCdmf5VZe"
   },
   "source": [
    "**Insight** - Graph 3: We can perceive from Graph 3 that the most prevalent level of education is \"Graduation\" level and the least one is \"Basic\", The No-Response:Response Ratio of all Levels is very high, meaning that most people DO NOT respond \n",
    "Orange - No Response\n",
    "Green - Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4D5IfkoQpsB"
   },
   "source": [
    "## 2. KMEANS (35 points)\n",
    "\n",
    "In this exercise, you will implement K-means clustering on a comprehensive customer dataset, to identify distinct customer segments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CuMf37LmjLEL"
   },
   "source": [
    "*   Load the data again.\n",
    "*   Scale the data using minmax scaler (2 points).\n",
    "*   Encode categorical variables (3 points).\n",
    "*   Apply k-Means algorithm on the 'MntMeatProducts' and 'MntWines' features using n_clusters=5\n",
    " (10 points).\n",
    "*   Visualize the clusters (5 points).\n",
    "*   Apply k-Means algorithm on all features and find the best k using 2 methods (10 points).\n",
    "*   Visualize the methods (5 points).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B8Z3hdGKQpaR"
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.compose import make_column_transformer\n",
    "import pandas as pd\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "# Provide the path to your Excel file\n",
    "path = 'C:\\\\Users\\\\Gal.Shani\\\\Documents\\\\Noam files\\\\Machine Learning\\\\HW_1\\\\customer_segmentation.csv'\n",
    "# # Load data into a DataFrame\n",
    "df = pd.read_csv(path)\n",
    "# Get the size of the DataFrame\n",
    "rows, columns = df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANNPxKACQnwL"
   },
   "outputs": [],
   "source": [
    "# Scale the data using MinMaxScaler, \n",
    "rows, columns = df.shape\n",
    "    # Scale the data using MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "    # fit and transform the data\n",
    "\n",
    "    # Identify numerical and non-numerical columns and other ones\n",
    "    #13\n",
    "# numerical_columns = ['Year_Birth', 'Income', 'Recency', 'MntWines', 'MntFruits', 'MntMeatProducts',\n",
    "#                          'MntFishProducts', 'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases',\n",
    "#                          'NumStorePurchases', 'NumWebVisitsMonth']\n",
    "# with ID\n",
    "numerical_columns = ['Year_Birth', 'Income', 'Recency', 'MntWines', 'MntFruits', 'MntMeatProducts',\n",
    "                         'MntFishProducts', 'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases',\n",
    "                         'NumStorePurchases', 'NumWebVisitsMonth']\n",
    "    #13\n",
    "categorical_columns = ['Education', 'Marital_Status', 'Kidhome', 'Teenhome', 'AcceptedCmp3', 'AcceptedCmp4',\n",
    "                           'AcceptedCmp5', 'AcceptedCmp1', 'AcceptedCmp2', 'Complain', 'Z_CostContact', 'Z_Revenue',\n",
    "                           'Response']\n",
    "# other_columns = ['ID','Dt_Customer'] \n",
    "# without ID\n",
    "other_columns = ['ID','Dt_Customer']\n",
    "    #non_numerical_columns = [col for col in df.columns if col not in numerical_columns]\n",
    "  #  non_numerical_data = df[non_numerical_columns]\n",
    "other_data = df[other_columns]\n",
    "numerical_data = df[numerical_columns]\n",
    "categorical_data = df[categorical_columns]\n",
    "\n",
    "scaled_numerical_data = pd.DataFrame(min_max_scaler.fit_transform(numerical_data), columns=numerical_data.columns)\n",
    "\n",
    "    # Concatenate the scaled numerical data with the non-numerical data\n",
    "scaled_df = pd.concat([other_data, scaled_numerical_data], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    # Encode categorical variables\n",
    "encoder = OneHotEncoder()\n",
    "    # 13\n",
    "categorical_columns = ['Education', 'Marital_Status', 'Kidhome', 'Teenhome', 'AcceptedCmp3', 'AcceptedCmp4',\n",
    "                           'AcceptedCmp5', 'AcceptedCmp1', 'AcceptedCmp2', 'Complain', 'Z_CostContact', 'Z_Revenue',\n",
    "                           'Response']\n",
    "    # Fit and transform the categorical data\n",
    "   # encoded_categorical_data = encoder.fit_transform(categorical_data)\n",
    "encoded_categorical_data = encoder.fit_transform(categorical_data).toarray()\n",
    "\n",
    "#     print(\"Categorical data: Num of cols \" + str(categorical_data.shape[1]))\n",
    "#     print(categorical_data)\n",
    "\n",
    "#     print(\"ENCODED Categorical data: Num of cols \" + str(encoded_categorical_data.shape[1]))\n",
    "#     print(encoded_categorical_data)\n",
    "\n",
    "    # Get the feature names\n",
    "#feature_names = encoder.get_feature_names(input_features=categorical_columns)\n",
    "\n",
    "categories = [f\"{col}_{category}\" for col, cats in zip(categorical_data.columns, encoder.categories_) for category in cats]\n",
    "\n",
    "   # feature_names_array = np.array(encoder.categories_).ravel()\n",
    "feature_names_array = np.array(encoder.categories_, dtype=object).ravel()\n",
    "#     print(\"Feature names: \" , feature_names_array)\n",
    "\n",
    "    # Create a DataFrame with the one-hot encoded features and feature names\n",
    "# encoded_df = pd.DataFrame(encoded_categorical_data, columns=feature_names)\n",
    "encoded_df = pd.DataFrame(encoded_categorical_data, columns=categories)\n",
    "#     print(\"Encoded variables: \\n\")\n",
    "#     print(encoded_categorical_data)\n",
    "#     print(\"encoded_df: Num of cols \" + str(encoded_df.shape[1]))\n",
    "    #encoded_df = pd.get_dummies(df, columns=['categorical_columns', ])\n",
    "    #print(\"encoded df size \"  + str(encoded_df.size()))\n",
    "\n",
    "\n",
    "    # Concatenate the encoded categorical variables with the scaled data\n",
    "final_data = pd.concat([scaled_df, encoded_df], axis=1)\n",
    "# final_data.to_csv('pre_processed_customer_segmentation.csv', index=False)\n",
    "    # Display the final DataFrame\n",
    "print(\"Final Data:\")\n",
    "print(final_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aigS0ZlGlBPn"
   },
   "outputs": [],
   "source": [
    "# Apply k-Means on the 'MntWines' and 'MntMeatProducts' features with n_clusters=5\n",
    "wines_products_cols = ['MntWines', 'MntMeatProducts']\n",
    "extracted_data_set = final_data[wines_products_cols]\n",
    "res = KMeans(n_clusters = 5, random_state = 0,n_init = 1)\n",
    "res.fit(extracted_data_set)\n",
    "# Visualize the clusters\n",
    "sns.scatterplot(data = extracted_data_set, x = 'MntWines', y = 'MntMeatProducts', hue = res.labels_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfPU_uMSpZ9l"
   },
   "source": [
    "### Elbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMPFkb91pZDD"
   },
   "outputs": [],
   "source": [
    "# Define the number of clusters to test (you can choose a range)\n",
    "k_vals = range(1, 10)\n",
    "df_copy = final_data.copy()\n",
    "df_copy.fillna(0, inplace=True)\n",
    "df_copy_no_date = df_copy.drop(columns=['Dt_Customer'])\n",
    "df_no_id_no_date = df_copy_no_date.drop(columns=['ID'])\n",
    "# Initialize an empty list to store the variance explained for each k\n",
    "variance_per_k = []\n",
    "\n",
    "# Fit KMeans with different values of k and calculate variance explained\n",
    "for k in k_vals:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0,n_init=10)\n",
    "    kmeans.fit(df_copy_no_date)\n",
    "    variance_per_k.append(kmeans.inertia_)  # kmeans.inertia_ gives the variance explained by the model\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(k_vals, variance_per_k, marker='o')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.text(3, -2, 'Best K using the Elbow Method: K=2', fontsize=12, ha='center')\n",
    "\n",
    "# # Show the plot\n",
    "# plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSev7vbfqTw7"
   },
   "source": [
    "### Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-NuZmpIsJNA"
   },
   "outputs": [],
   "source": [
    "# Specify the range of clusters (k) to try\n",
    "k_values = range(2, 10)\n",
    "\n",
    "# List to store silhouette scores for each k\n",
    "silhouette_scores = []\n",
    "df_no_id_no_date_copy1 = df_no_id_no_date.copy()\n",
    "# Iterate over different values of k\n",
    "for k in k_values:\n",
    "    # Initialize KMeans model\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0,n_init = 10)\n",
    "\n",
    "    # Fit the model and obtain cluster labels\n",
    "    cluster_labels = kmeans.fit_predict(df_no_id_no_date_copy1)\n",
    "\n",
    "    # Calculate silhouette score\n",
    "    silhouette_avg = silhouette_score(df_no_id_no_date_copy1, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "# Plot the silhouette scores for each k\n",
    "plt.plot(k_values, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score for Different Values of k')\n",
    "plt.text(3, -2, 'Best K using the Sillhouette Method: K=2', fontsize=12, ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wefQVfhtCKs"
   },
   "source": [
    "## 3. PCA (15 points)\n",
    "In this exercise, you will implement PCA:\n",
    "*   With n_components = 2 (5 points).\n",
    "*   Visualize the PCA (5 points).\n",
    "*   Find the variance explined in this PCA (5 points).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVdzsODntCpd"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def do_PCA(df, n_components=2):\n",
    "    # Assuming 'ID' is one of the columns to drop\n",
    "    df_no_id_no_date = df.drop(columns=['ID'])\n",
    "    \n",
    "    # Create a DataFrame with the principal components\n",
    "    pca = PCA(n_components)\n",
    "    pca_df = pca.fit_transform(df_no_id_no_date)\n",
    "\n",
    "    # Visualize the PCA\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(pca_df[:, 0], pca_df[:, 1], cmap='viridis', edgecolor='k', s=60)\n",
    "    plt.title('PCA of Customer Segmentation Pre-Processed Dataset: \\n')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.show()\n",
    "\n",
    "    # Find the variance explained in this PCA\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    total_variance_explained = np.sum(explained_variance_ratio)\n",
    "\n",
    "    # Display explained variance\n",
    "    print(f\"Variance explained by PC1: {explained_variance_ratio[0]*100:.2f}%\")\n",
    "    print(f\"Variance explained by PC2: {explained_variance_ratio[1]*100:.2f}%\")\n",
    "    print(f\"Total variance explained by both components: {total_variance_explained*100:.2f}%\")\n",
    "\n",
    "    return pca_df, explained_variance_ratio\n",
    "\n",
    "# Now, you can use this function with your DataFrame\n",
    "pca_result, variance_explained = do_PCA(df_copy_no_date, n_components=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K8fTt4Kl00C7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQTAlkFn040l"
   },
   "source": [
    "**Q**: What is the variance explained in the 2 component PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmuwM57w1XY2"
   },
   "source": [
    "**A**: The Variance Explained by this 2 component PCA means that PC1 is responsible for 16.15% of the TOTAL variance observed in the data samples, while PC2 is responsible for the remaining 14.73%.\n",
    "However, these 2 components explain only 30.88% of the total variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FBNDN4M1_5R"
   },
   "source": [
    "## 4. PCA & Kmeans (20 points)\n",
    "This time, we will use the PCA data for the kmeans model.\n",
    "*   Run PCA with n_components = 2 (5 points)\n",
    "*   Find the best k for kmeans (5 points)\n",
    "*   Plot the best clusters (5 points)\n",
    "*   Answer the question (5 points)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g567KKzSzWYE"
   },
   "outputs": [],
   "source": [
    "# Adjust n_components as needed\n",
    "df_no_date_copy2 = df_copy_no_date.copy()\n",
    "df_no_date__no_id = df_no_date_copy2.copy().drop(columns=['ID'])\n",
    "n_components_pca = 2\n",
    "\n",
    "# Create a DataFrame with the principal components\n",
    "pca = PCA(n_components_pca)\n",
    "pca_result = pca.fit_transform(df_no_date__no_id)\n",
    "\n",
    "# Apply PCA with n_components=2\n",
    "pca_kmeans = PCA(n_components=2)\n",
    "pca_result_kmeans = pca_kmeans.fit_transform(df_no_date__no_id)\n",
    "\n",
    "# Create a DataFrame with the principal components\n",
    "pca_df_kmeans = pd.DataFrame(data=pca_result_kmeans, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Normalize PCA data to have values between 0 and 1\n",
    "scaler_kmeans = MinMaxScaler()\n",
    "pca_df_normalized_kmeans = pd.DataFrame(scaler_kmeans.fit_transform(pca_df_kmeans), columns=['PC1', 'PC2'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNRbRYwQ3F1p"
   },
   "source": [
    "### Elbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MacHJeVg3DUM"
   },
   "outputs": [],
   "source": [
    "# Find the best k using the elbow method with normalized data\n",
    "k_values_kmeans = range(1, 11)\n",
    "inertia_values_kmeans = []\n",
    "\n",
    "for k_kmeans in k_values_kmeans:\n",
    "    kmeans_kmeans = KMeans(n_clusters=k_kmeans, random_state=0)\n",
    "    kmeans_kmeans.fit(pca_df_normalized_kmeans)\n",
    "    inertia_values_kmeans.append(kmeans_kmeans.inertia_)\n",
    "\n",
    "inertia_values_kmeans = np.array(inertia_values_kmeans) / 500\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.plot(k_values_kmeans, inertia_values_kmeans, marker='o')\n",
    "plt.title('Elbow Method for Optimal k (on Normalized PCA data)')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia (scaled by 500)')\n",
    "plt.show()\n",
    "\n",
    "# Determine the optimal k based on the elbow in the plot\n",
    "optimal_k_kmeans_elbow = np.argmin(np.diff(inertia_values_kmeans)) + 2  # Add 2 because the index starts from 0\n",
    "\n",
    "# Print the optimal k for KMeans using Elbow Method\n",
    "print(f\"Optimal k using Elbow Method for KMeans: {optimal_k_kmeans_elbow}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G79RYqGx3SX5"
   },
   "source": [
    "### Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IHgwpNnK3MHw"
   },
   "outputs": [],
   "source": [
    "# Silhouette Score\n",
    "# Specify the range of clusters (k) to try\n",
    "k_values_silhouette = range(2, 10)\n",
    "\n",
    "# List to store silhouette scores for each k\n",
    "silhouette_scores = []\n",
    "\n",
    "# Iterate over different values of k\n",
    "for k_silhouette in k_values_silhouette:\n",
    "    # Initialize KMeans model\n",
    "    kmeans_silhouette = KMeans(n_clusters=k_silhouette, random_state=0, n_init=10)\n",
    "\n",
    "    # Fit the model and obtain cluster labels\n",
    "    cluster_labels_silhouette = kmeans_silhouette.fit_predict(pca_df_normalized_kmeans)\n",
    "\n",
    "    # Calculate silhouette score\n",
    "    silhouette_avg_silhouette = silhouette_score(pca_df_normalized_kmeans, cluster_labels_silhouette)\n",
    "    silhouette_scores.append(silhouette_avg_silhouette)\n",
    "\n",
    "# Plot the silhouette scores for each k\n",
    "plt.plot(k_values_silhouette, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score for Different Values of k')\n",
    "plt.show()\n",
    "\n",
    "# Print the optimal k for KMeans using Silhouette Score\n",
    "optimal_k_kmeans_silhouette = np.argmax(silhouette_scores) + 2\n",
    "print(f\"Optimal k using Silhouette Method for KMeans: {optimal_k_kmeans_silhouette}\")\n",
    "\n",
    "# Assuming the necessary libraries are already imported\n",
    "\n",
    "# Task 4: KMeans with Optimal k (using Silhouette Score)\n",
    "# Use the optimal k obtained from the Silhouette Score\n",
    "optimal_k_silhouette = optimal_k_kmeans_silhouette\n",
    "\n",
    "# Initialize KMeans model with optimal k\n",
    "kmeans_optimal_silhouette = KMeans(n_clusters=optimal_k_silhouette, random_state=0, n_init=10)\n",
    "kmeans_optimal_silhouette.fit(pca_df_normalized_kmeans)\n",
    "\n",
    "# Add cluster labels to the normalized PCA DataFrame\n",
    "pca_df_normalized_kmeans['Cluster'] = kmeans_optimal_silhouette.labels_\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "for cluster in range(optimal_k_silhouette):\n",
    "    cluster_data = pca_df_normalized_kmeans[pca_df_normalized_kmeans['Cluster'] == cluster]\n",
    "    plt.scatter(cluster_data['PC1'], cluster_data['PC2'], label=f'Cluster {cluster + 1}')\n",
    "\n",
    "plt.scatter(kmeans_optimal_silhouette.cluster_centers_[:, 0], kmeans_optimal_silhouette.cluster_centers_[:, 1], c='red', marker='X', label='Centroids')\n",
    "plt.title('KMeans Clustering with Optimal k')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wr9Czzz74Zrc"
   },
   "source": [
    "**Q**: In our human eye, it's looks like we need 5 clusters. But both methods return 2. Why do you think kmeans returning 2 and not 5?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQKf6VCk4szz"
   },
   "source": [
    "**A**:###This question is cancelled!###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPuoIFUzQTyl"
   },
   "source": [
    "## 5. K-means Clustering Exercise (15 points)\n",
    "Manually divide a given set of points into clusters using the K-means clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFYTbVgESk5i"
   },
   "source": [
    "You are given the following two-dimensional points: <br>\n",
    "*   A:(2,2)\n",
    "*   B:(2,6)\n",
    "*   C:(3,7)\n",
    "*   D:(5,5)\n",
    "*   E:(6,2)\n",
    "*   F:(7,4)\n",
    "*   G:(8,7)\n",
    "\n",
    "**Tasks:**\n",
    "1.   **Visualize the Data**: Plot these points and label each point for easy identification (3 points).\n",
    "2.   **Choose Initial Cluster Centers**: Arbitrarily select three points as initial cluster centers (For example, you might choose points A and D) and **Answer the questions** (12 points).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "loZG5mEKVAqJ"
   },
   "outputs": [],
   "source": [
    "# Given two-dimensional points\n",
    "points = {'A': (2, 2), 'B': (2, 6), 'C': (3, 7), 'D': (5, 5), 'E': (6, 2), 'F': (7, 4), 'G': (8, 7)}\n",
    "\n",
    "# Visualize the Data\n",
    "plt.figure(figsize=(8, 6))\n",
    "for point, coordinates in points.items():\n",
    "    plt.scatter(coordinates[0], coordinates[1], label=point)\n",
    "\n",
    "plt.title('Visualization of Two-dimensional Points')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Convert points to a numpy array for KMeans\n",
    "data = np.array(list(points.values()))\n",
    "\n",
    "# K-means Clustering\n",
    "# Arbitrarily choose three points as initial cluster centers (A, D, and G)\n",
    "#initial_centers = np.array([points['A'], points['D'], points['G']])\n",
    "initial_centers = np.array([points['A'], points['B'], points['C']])\n",
    "\n",
    "# Perform K-means clustering\n",
    "kmeans = KMeans(n_clusters=3, init=initial_centers, n_init=1, random_state=0)\n",
    "kmeans.fit(data)\n",
    "\n",
    "# Add cluster labels to the points\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(3):\n",
    "    cluster_data = data[cluster_labels == i]\n",
    "    plt.scatter(cluster_data[:, 0], cluster_data[:, 1], label=f'Cluster {i + 1}')\n",
    "\n",
    "plt.scatter(initial_centers[:, 0], initial_centers[:, 1], c='red', marker='X', label='Initial Centers',s=10)\n",
    "plt.title('K-means Clustering of Two-dimensional Points')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIXBX71dV0sP"
   },
   "source": [
    "**Questions**:\n",
    "1.   How did the points group together in the final iteration?\n",
    "2.   Was choosing different initial cluster centers leading to different final clusters? Why might this happen?\n",
    "3.   Think of any real-world scenarios where K-means clustering could be useful?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a__DebSWTL-"
   },
   "source": [
    "**Answers**:\n",
    "\n",
    "Answer 1: The points grouped into clusters around the initial cluster centers (2,2), (5,5), and (8,7) in the final iteration.\n",
    "\n",
    "Answer 2: No, in this case, choosing different initial cluster centers did not lead to different final clusters. The algorithm converged to a solution where clusters were formed around (2,2), (5,5), and (8,7). However, it's important to note that in general, K-means is sensitive to initial cluster centers, and different initial centers might lead to different local minimum.\n",
    "\n",
    "Answer 3: K-means clustering can be useful in various real-world scenarios, such as: customer segmentation in marketing, image compression in computer vision, anomaly detection in cyber-security, and organizing documents in natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTyioTC96CSR"
   },
   "source": [
    "## 6. Bonus - Compute projection (5 points)\n",
    "**Notice:** No code required in this section.\n",
    "\n",
    "Given the next PCA projection matrix\n",
    "$\\begin{pmatrix}\n",
    "1 & 1\\\\\n",
    "2 & -1\n",
    "\\end{pmatrix}$\n",
    "And the correspoding egienvalues\n",
    "$(5, -1)$\n",
    "\n",
    "Compute the projection to one dimension of the next two vectors:\n",
    "\\begin{pmatrix}\n",
    "1 & 3\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "3 & 7\n",
    "\\end{pmatrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MuMpJEjm6Sm2"
   },
   "outputs": [],
   "source": [
    "# Projection of (1,3) onto 1 dimension: Vtranspose*v1 =  (7,2)*(1,2) = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection of (3,7) onto 1 dimension: Vtranspose*v2 = (17,-4)(1,-1) = 13"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Esbc-ZiCO60h",
    "r8ssPm39Q5-6",
    "RfPU_uMSpZ9l",
    "KSev7vbfqTw7",
    "-wefQVfhtCKs",
    "1FBNDN4M1_5R",
    "uNRbRYwQ3F1p",
    "G79RYqGx3SX5"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
